\chapter{Expermients and Methodology}

\section{Introduction}

This section describes the experiments and methodology used to evaluate the proposed solutions. 
The experiments were conducted on two different datasets: BrightKite and CitiBike, which are 
real-world datasets, and we included a synthetic dataset for parameter tuning.

\section{Datasets}

\subsection{BrightKite Data}

\subsubsection{Overview}

BrightKite was a location-based social network where users shared their locations by checking in at 
different places.

The dataset contains check-ins from users, and each check-in contains the user's ID, the location's ID, 
the timestamp, and the location's geographical coordinates.

BrightKite data was used by Thodoris Lykouris and Sergei Vassilvitskii in their paper "Competitive 
Caching with Machine Learned Advice" \cite{datasets-reference} and the dataset is publicly available
at \cite{brightkite-data}.

From this dataset, we created multiple instances of the caching problem, by considering each user 
as a tenant for our multi-tenant caching problem, location IDs as items or pages to be cached, and 
check-ins as item or page accesses. 

\subsubsection{Processing}

Similar to the work done by Thodoris Lykouris and Sergei Vassilvitskii in \cite{datasets-reference},
we selected the top users where the optimal cache algorithm (Belady's or clairvoyant) had more cache 
faults, to select the most challenging instances from the dataset.

Then we created 6 instances of the multi-tenant caching problem, by considering each of the most 
challenging users as a tenant, the locations as items to be cached, and the check-ins as item accesses.

We took 36 most challenging users from the dataset, and created 6 instances of the multi-tenant caching
problem, two of them with 4 tenants, two with 5 tenants, one with 8 tenants, and one with 10 tenants.

Information of the selected users, with number of accesses, number of unique items, and number of cache
faults for the optimal cache algorithm (with cache size equal to 50), rank and user ID, is shown in
Appendix \ref{appendix:brightike-top-36-users}.

The number of accesses per user is from 1300 to 2100, with most of them having 2100. The number of
unique items per user is from 300 to 1300, with many of them having around 600. The number of cache
faults for the optimal cache algorithm is from 10 to 164, with most of them having around 20 or 30.

To make the cache size recommendations per tenant, we started by some initial value, and kept increasing 
it until the number of cache faults for the optimal cache algorithm was less than some threshold, in this
case 40.

\subsection{CitiBike Data}

\subsubsection{Overview}

CitiBike is a popular bike-sharing service in New York City. We consider citibike trip histories, 
in which each ride is a cache item access. Experiments were made with monthly data from 2023, 
the dataset is publicly available at \cite{citibike-data}.

The dataset contains information about each trip, the start and end station IDs, if the bike
was electric or classic, data about the rider, if the rider is a member or a casual rider, 
the start date and time that was used to sort the data, and more information not used in the 
experiments.

CitiBike data was also used by Thodoris Lykouris and Sergei Vassilvitskii in their paper "Competitive 
Caching with Machine Learned Advice" \cite{datasets-reference}.

We made artificial tenants by considering if the bike was electric or classic, and if the rider 
was a member or a casual rider, therefore we have 4 tenants: electric members, electric casuals,
classic members, and classic casuals.

\subsubsection{Processing}

We made three experiments: 

\begin{itemize}
    \item In the first one, we considered the pair (start\_station\_id, end\_station\_id) to be 
    the items to be cached, so that two trips are only considered the same if they have the 
    same start and end stations.
    \item In the second one, we ranked start stations to numbers from 1 to the number of unique
    start stations, and end stations to numbers from 1 to the number of unique end stations, so
    and then considered the pair $(\lfloor \frac{start\_station\_rank}{5} \rfloor, \lfloor \frac{end\_station\_rank}{5} \rfloor)$ 
    to be the items to be cached.
    \item In the third one, we considered the start station ID to be the item to be cached.
\end{itemize}

The number of unique items and number of accesses in each experiment is shown on the following tables.

\newpage

\begin{table}[ht]
    \centering
    \small
    \caption{Summary per tenant in the CitiBike experiment 1.}
    \csvautotabular{data/citibike_exp_1_case_2_summary.csv}
    \label{tab:citibike-exp-1-case-2-summary}
\end{table}

\begin{table}[ht]
    \centering
    \small
    \caption{Summary per tenant in the CitiBike experiment 2.}
    \csvautotabular{data/citibike_exp_2_case_3_summary.csv}
    \label{tab:citibike-exp-2-case-3-summary}
\end{table}

\begin{table}[ht]
    \centering
    \small
    \caption{Summary per tenant in the CitiBike experiment 3.}
    \csvautotabular{data/citibike_exp_3_case_4_summary.csv}
    \label{tab:citibike-exp-3-case-4-summary}
\end{table}

To make the cache size recommendations per tenant, we started by some initial value selected differently
for each experiment, based on the number of unique items and number of accesses, and kept increasing it 
until the number of cache faults for the optimal cache algorithm was less than some threshold, based 
on percentage of the number of accesses.

\subsection{Random Data}


\subsubsection{Overview}

Random data was used for hyperparameter tuning, testing, and validation of the algorithms, 
but not in the experiments.

It has been shown that when the probability distribution of the data access pattern is constant 
over time, LFU yields the highest performance \cite{lfu-highest-perf-zipf} 
\cite{lfu-highest-perf-inproc} \cite{tiny-lfu}. Therefore, we did not generate data using only 
one probability distribution, as this would favor LFU and not take into account the problem of 
changing access patterns over time.

\subsubsection{Generation}

To generate random data, we used the following probability distributions:

\begin{itemize}
    \item Zipfian distribution, since a popular assumption is that cache workloads follow it 
    \cite{zipf-dist-cache-1} \cite{zipf-dist-cache-2} \cite{zipf-dist-cache-3} 
    \cite{lfu-highest-perf-zipf}.
    \item Pareto distribution, since it has been shown that storage and internet traffic traces 
    are well modeled by it \cite{pareto-dist-workload} \cite{pareto-dist-workload-2} 
    \cite{pareto-dist-workload-3}.
    \item Normal distribution; we used it less than the other distributions since it is not 
    commonly used in cache workloads, but it was used in \cite{memory-aware-buffer-pool-manager}.
    \item Uniform distribution; we used it less than the other distributions since it is not 
    commonly used in cache workloads, but it was used in \cite{memory-aware-buffer-pool-manager}.
\end{itemize}

To address the problem of changing access patterns over time, in some cases, we generated data 
using one distribution for a period of time, and then changed the distribution to another one. 
In many cases, the distribution was the same, but we renumbered the most accessed items.

To address the problem of correlated accesses, in some cases, when generating data for the 
tenants, we generated correlated accesses to some items, causing them to be accessed together 
during certain periods of time.

To handle multi tenancy, in some cases we randomly merged the data of the tenants, and in 
other cases we splitted the data of each tenant into chunks, and then randomly merged the
chunks.

\section{Testing and Validation}

\subsection{Data Validation}

In order to ensure the integrity of the data, we validated each case for the experiments and 
the randomly generated data.

For each case, we checked that each tenant had the correct number of accesses and the correct 
number of unique items, both by script and by visually plotting the data to ensure it was 
correctly generated.

Additional integrity checks related to buffer sizes, priority levels, and tenant information 
were also performed.

\subsection{Algorithm Validation}

In order to validate the algorithms, we simulated the cache eviction process for each case, 
keeping track of the cache state and the cache faults, and performing integrity checks related 
to the cache state, cache hits, and faults.

Checks related to the tenant selection policy were performed to ensure that each tenant had at 
least the minimum buffer size promised at any time, did not exceed the maximum buffer size 
promised at any time, and was selected according to the tenant selection policy.

Additional checks related to the cache eviction policy were performed to ensure that the 
eviction policy was correctly implemented and worked as expected.

\section{Experiments}

\subsection{Hit Ratio Per Cache Size Experiment}

\subsection{Fault Score Experiment}

\lipsum[1-3]

\lipsum[1-1] \ref{fig:figA-1}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3  \textwidth]{dummy.png}
    \caption{Lorem ipsum dolor sit amet}
    \label{fig:figA-3}
\end{figure}

\lipsum[1-1] \ref{fig:figB-1}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{dummy.png}
    \caption{Lorem ipsum dolor sit amet}
    \label{fig:figB-3}
\end{figure}


\lipsum[1-1] \cite{reference-1}


\lipsum[1-1] \cite{reference-2}


\lipsum[1-1] \cite{reference-3}


\begin{table}[ht!]
    \centering
    \begin{tabular}{c c c}
        \hline
        Column 1 & Column 2 & Column 3 \\
        \hline
        Item 1   & Item 2   & Item 3   \\
    \end{tabular}
    \caption{Lorem ipsum dolor sit amet}
    \label{tab:tabA-3}
\end{table}

\lipsum[1-1] \ref{tab:tabA-1}

\begin{table}[ht!]
    \centering
    \begin{tabular}{c c c}
        \hline
        Column 1 & Column 2 & Column 3 \\
        \hline
        Item 1   & Item 2   & Item 3   \\
    \end{tabular}
    \caption{Lorem ipsum dolor sit amet}
    \label{tab:tabB-3}
\end{table}

\lipsum[1-1] \ref{tab:tabB-1}

\lipsum[1-2]